
########################
HIVE Query execution:
########################

SQL queries are submitted to Hive and they are executed as follows:

    -- Hive compiles the query.

    -- An execution engine, such as Tez or MapReduce, executes the compiled query.

    -- The resource manager, YARN, allocates resources for applications across the cluster.

    -- The data that the query acts upon resides in HDFS (Hadoop Distributed File System). Supported data formats are ORC, AVRO, Parquet, and text.

    -- Query results are then returned over a JDBC/ODBC connection.
    

####################
Hive Architecture
####################

- Hive Clients

- Hive services - Hiveserver2, beeline, Hive Driver(receives hql, creates session handles for the query and sends to compiler), 
  Hive Compiler (comiles, typecheks, parses with metdata and generates execution plan called DAG where each stage is MRjob, HDFS ops, metastore ops), Optimizer, Execution Engine, 
  Metastore (Metastore is a central repository that stores the metadata information about the structure of tables and partitions, including column and column type information.
  It also stores information of serializer and deserializer, required for the read/write operation, and HDFS files where data is stored. 
  This metastore is generally a relational database.)

- Processing Framework and Resource Management (Hive internally uses a MapReduce framework as a defacto engine for executing the queries.)

- Distributed Storage (Hive is built on top of Hadoop, so it uses the underlying HDFS)


#################
Working of Hive:
#################	
	executeQuery
	getPlan (driver accepts the query, creates a session handle for the query, and passes the query to the compiler for generating the execution plan.)
	getMetaData (compiler sends metadeta request to metastore)
	sendMetaData (metastore sends the metadata to the compiler)
	sendPlan (compiler then sends the generated execution plan to the driver)
	executePlan (river sends the execution plan to the execution engine for executing the plan)
	submit job to MapReduce ()
  
  
 ##############################
 Advanced Interview Questions
 ##############################
 
 1) How to manage multiple hive batch jobs:
  
	Yarn queue mapping..
	Configure a queue for batch processing
	You can configure the capacity scheduler queues to scale a Hive batch job for your environment. 
	YARN uses the queues to allocate Hadoop cluster resources among users and groups.

 2) What is managed table and external table in hive:
 
	Managed tables are Hive owned tables where the entire lifecycle of the tablesâ€™ data are managed and controlled by Hive. 
	All the write operations to the Managed tables are performed using Hive SQL commands. 
	If a Managed table or partition is dropped, the data and metadata associated with that table or partition are deleted. 
	The transactional semantics (ACID) are also supported only on Managed tables.
	
	External tables are tables where Hive has loose coupling with the data.
	The writes on External tables can be performed using Hive SQL commands but data files can also be accessed and managed by processes outside of Hive. 
	If an External table or partition is dropped, only the metadata associated with the table or partition is deleted but the underlying data files stay intact. 
	A typical example for External table is to run analytical queries on HBase or Druid owned data via Hive, where data files are written by HBase or Druid and Hive reads them for analytics.
	Hive supports replication of External tables with data to target cluster and it retains all the properties of External tables.
	The data files permission and ownership are preserved so that the relevant external processes can continue to write in it even after failover. 

 
 3) What are Hive Partitions & Buckets:
 
 	Partitions:Hive Partitions is a way to organizes tables into partitions by dividing tables into different parts based on partition keys.
		   Partition is helpful when the table has one or more Partition keys (columns).
		   Partition keys are basic elements for determining how the data is stored in the table. 
		   
		   Creation of partition table:
		   	create table state_part(District string,Enrolments string) PARTITIONED BY(state string);
		   
		   For partition we have to set this property:
		   	set hive.exec.dynamic.partition.mode=nonstrict

		   If we take state column as partition key and perform partitions on that India data as a whole, 
		   we can able to get Number of partitions (38 partitions) which is equal to number of states (38) present in India.
		   Such that each state data can be viewed separately in partitions tables. 
 	
	Buckets: Buckets in hive is used in segregating of hive table-data into multiple files or directories. it is used for efficient querying.
		 The data i.e. present in that partitions can be divided further into Buckets.
		 The division is performed based on Hash of particular columns that we selected in the table.
		 Buckets use some form of Hashing algorithm at back end to read each record and place it into buckets
		 In Hive, we have to enable buckets by using the set.hive.enforce.bucketing=true;
		 
		 hive> create table samlpe_tableame {first_name string,
		 				     job_id	int,
						     department string,
						     salary 	string,
						     country	string }
					clustered by (country) into 4 buckets
					row format delimited
					fields terminated by ',';
					
			    We are creating sample_bucket with column names such as first_name, job_id, department, salary and country.
			    We are creating 4 buckets overhere.
			    Once the data gets loaded, it automatically places the data into 4 buckets.
		 
		 hadoop-2.2.0/bin$ ./hadoop fs -ls /user/hive
		 --rwx----            /user/hive/sampledb/sample_tablename/000000_0
		 --rwx----            /user/hive/sampledb/sample_tablename/000000_1
		 --rwx----            /user/hive/sampledb/sample_tablename/000000_2
		 --rwx----            /user/hive/sampledb/sample_tablename/000000_3
			
		# as seen above data is transferred in to 4 buckets _0, _1, _2, _3
 
 4) What is Orderby, Sortby, DistributeBy. ClusterBy in hive:
 	
	ORDER BY x: guarantees global ordering, but does this by pushing all data through just one reducer. 
		    This is basically unacceptable for large datasets. You end up one sorted file as output.
	
	SortBy: Sort by clause performs on column names of Hive tables to sort the output.
		We can mention DESC for sorting the order in descending order and mention ASC for Ascending order of the sort.
		In this sort by it will sort the rows before feeding to the reducer. Always sort by depends on column types.
		For instance, if column types are numeric it will sort in numeric order if the columns types are string it will sort in lexicographical order.
		SORT BY x: orders data at each of N reducers, but each reducer can receive overlapping ranges of data. You end up with N or more sorted files with overlapping ranges.
		
	DistributeBy: Distribute BY clause used on tables present in Hive. Hive uses the columns in Distribute by to distribute the rows among reducers. 
		All Distribute BY columns will go to the same reducer.
		It ensures each of N reducers gets non-overlapping ranges of column.
		It doesn't sort the output of each reducer
		
	ClusterBy: Cluster By used as an alternative for both Distribute BY and Sort BY clauses in Hive-QL.
		Cluster BY clause used on tables present in Hive. Hive uses the columns in Cluster by to distribute the rows among reducers. 
		Cluster BY columns will go to the multiple reducers.
		It ensures sorting orders of values present in multiple reducers.
		For example, Cluster By clause mentioned on the Id column name of the table employees_guru table.
		The output when executing this query will give results to multiple reducers at the back end. 
		But as front end it is an alternative clause for both Sort By and Distribute By.
		This is actually back end process when we perform a query with sort by, group by, and cluster by in terms of Map reduce framework.
		So if we want to store results into multiple reducers, we go with Cluster By. 
		
		CLUSTER BY x: ensures each of N reducers gets non-overlapping ranges, then sorts by those ranges at the reducers. 
		This gives you global ordering, and is the same as doing (DISTRIBUTE BY x and SORT BY x). You end up with N or more sorted files with non-overlapping ranges.
		
		So CLUSTER BY is basically the more scalable version of ORDER BY.

 5) How can you change the execution engine of hive:
 
 	By setting the property hive.execution.engine = 'Tez' during the local session or setting it up globally we can set our required execution engine.
 
 
 
 
